# -*- coding: utf-8 -*-
"""WordCloud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B_VdlLMsCv-9yNTeP3n0UemQaqPgLYbY

***Notebook* para coleta do twitter e criação de wordclouds**.
"""

# wordcloud: https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html
# Tweepy: https://docs.tweepy.org/en/v4.10.0/


import tweepy
import csv
import json
import pandas as pd
import os
from wordcloud import WordCloud
from wordcloud import ImageColorGenerator
from wordcloud import STOPWORDS
import matplotlib.pyplot as plt
import re

access_token = "1400895473389555716-68Cmo5V4I5oyVkoG2OClLunwbcpMZi"
access_token_secret = "i1tKGavHC5Nj0TaAptv8UBLDgTjMUaVHOXmB1xiC9dgOd"
consumer_key = "KVcLaueo5lqxfTt7ZHjik6tEa"
consumer_secret = "DjdHgEuA4lT5nOq4gwZEd3RrkxN7lyNInsUaQ4DWvqkuA2Oq0A"
bearer_token = "AAAAAAAAAAAAAAAAAAAAAEeOQQEAAAAA9T8aDYPek7lSfYVe6F4WerqICBg%3DY1fWmANkCv4FposOpgrAYdoZbqeuHNxONFMNZE3QQuAe9GKwsz"

auth = tweepy.auth.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.Client(bearer_token=bearer_token, consumer_key=consumer_key, consumer_secret=consumer_secret, access_token=access_token, access_token_secret=access_token_secret)

query_twitter = "from:jairbolsonaro (Decreto OR quarentena) lang:pt -is:retweet"

start_time = '2025-01-01T00:00:00Z'
end_time = '2026-01-31T00:00:00Z'

tweet_fields=['created_at','author_id','text', 'public_metrics']

response = api.search_all_tweets(query=query_twitter, tweet_fields=tweet_fields, max_results=500, start_time=start_time, end_time=end_time)

listTweets = list()

try:
  count = 0
  for tweet in response.data:
    count = count+1
    setTweetData = {
          'created_at': tweet.created_at,
          'author_id': tweet.author_id,
          'text': tweet.text,
          'metrics': tweet.data
      }
    listTweets.append(setTweetData)
except:
  pass

#politicos_df = pd.DataFrame(listTweets)
politicos_df = pd.read_csv("files/comments_info.csv")
string = " ".join(df_comentarios['textDisplay'].astype(str))


politicos_df['created_at'] = politicos_df['created_at'].dt.tz_localize(None)
politicos_df['author_id'] = politicos_df['author_id'].astype(object)
politicos_df


# Montar nuvem de palavras

df_nuvem = politicos_df

# Retira os links das postagens e trata os textos
df_nuvem['cleanText'] = politicos_df['text'].apply(lambda x: re.split('https:\/\/.*', str(x))[0])
string = pd.Series(df_nuvem['cleanText'].values).str.cat(sep=' ')

# Retira os caracteres especiais"
caracteres = "#@\n"
for i in range(len(caracteres)):
  string = string.replace(caracteres[i],"")

stopwords = set(STOPWORDS)

# Caso precise remover alguma palavra
stopwords.update(["e", "o", "que", "pelo", "pela", "para", "com", "dos", "das", "nikolas", "ferreira"])



wordcloud = WordCloud(width=1600, stopwords=stopwords,height=800,max_font_size=200,max_words=300,collocations=False, background_color='white').generate(string)
plt.figure(figsize=(40,30))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

# Executar uma vez

# determining the name of the file
file_name = 'politics_1.xlsx'


# saving the excel
politicos_df.to_excel(file_name, encoding='utf-8')
print('Dataframe salvo em Excel')