import pandas as pd
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import os
import re

# --- 1. CONFIGURA√á√ÉO DE CAMINHOS ---
BASE_PATH = r'C:\AMS_final\files'
SAVE_PATH = r'C:\AMS_final\apresentacao_visual\wordclouds_refinadas2'
ARQUIVO = os.path.join(BASE_PATH, 'comments_v16_dicionarioMod_final.csv')

if not os.path.exists(SAVE_PATH):
    os.makedirs(SAVE_PATH)

# --- 2. MEGA LISTA DE STOPWORDS REFINADA ---
# Inclui conectivos, pronomes e ru√≠dos t√≠picos de YouTube (com e sem acento)
STOPWORDS_BASE = set(STOPWORDS)
RU√çDO_EXTRA = {
    'e', '√©', 'n√£o', 'nao', 'que', 'q', 'de', 'do', 'da', 'em', 'um', 'uma', 'os', 'as', 
    'para', 'pra', 'com', 'no', 'na', 'nos', 'nas', 'por', 'mais', 'mas', 'foi', 'vai', 
    'vou', 'est√°', 't√°', 'tem', 'tinha', 'ser', 'seu', 'sua', 'seus', 'suas', 'meu', 
    'minha', 'meus', 'minhas', 'ele', 'ela', 'eles', 'elas', 'voc√™', 'vc', 'vcs', 
    'isso', 'isto', 'aquele', 'aquela', 'quem', 'qual', 'quando', 'onde', 'como', 
    'porque', 'porqu√™', 'pq', 'at√©', 'mesmo', 'tamb√©m', 'ainda', 's√≥', 'so', 'tudo', 
    'pode', 'podem', 'fazer', 'disse', 'diz', 'falou', 'ver', 'ter', 'tenho', 't√™m', 
    'seja', 'era', 'disso', 'daqui', 'aqui', 'ai', 'a√≠', 'l√°', 'la', 'est√£o', 'estao', 'esta','trump', 'esse','pais',
}
STOPWORDS_PT = STOPWORDS_BASE.union(RU√çDO_EXTRA)

# Stopwords espec√≠ficas para "limpar" o tema e focar nos adjetivos e gatilhos
STOPWORDS_TEMA = {
    'EUA-Brasil': {'brasil', 'eua', 'americano', 'brasileiro', 'governo', 'presidente'},
    'EUA-Venezuela': {'venezuela', 'eua', 'maduro', 'povo', 'ditador', 'nicolas'},
    'Polarizacao-Brasil-Interno': {'brasil', 'lula', 'bolsonaro', 'pt', 'governo', 'povo'},
    'Russia-Ucrania': {'russia', 'ucrania', 'china', 'esta', 'muito', 'mundo', 'todo', 'essa', 'brasil', 'putin', 'zelensky', 'guerra', 'pais', 'russo'},
    'EUA-China': {'china', 'esta', 'trump', 'muito', 'chinese', 'chineses', 'eua', 'paise', 'estado','chine', 'paises', 'chines', 'americano', 'esse' 'chines', 'economia', 'mundo', 'pais', 'xi'},
    'EUA-Ira': {'ira', 'eua', 'guerra', 'iraniano', 'americano', 'pais'},
    'EUA-Europa': {'europa', 'eua', 'uniao', 'europeia', 'paises', 'europeu'},
    'Lideranca-EUA-Hegemonia': {'eua', 'mundo', 'potencia', 'lider', 'hegemonia'},
    'Outros': set()
}

def gerar_nuvens_limpas():
    print("‚òÅÔ∏è Iniciando Limpeza e Gera√ß√£o de WordClouds...")
    try:
        df = pd.read_csv(ARQUIVO, usecols=['text_cleaned', 'tema_geopolitico'])
    except FileNotFoundError:
        print(f"‚ùå Arquivo n√£o encontrado em {ARQUIVO}")
        return

    # Processamento por Tema
    for cat in df['tema_geopolitico'].unique():
        print(f"üì¶ Filtrando tema: {cat}")
        df_cat = df[df['tema_geopolitico'] == cat]
        
        # Amostragem para performance (mantendo 30k coment√°rios por tema)
        tamanho = min(30000, len(df_cat))
        texto = " ".join(df_cat['text_cleaned'].astype(str).sample(n=tamanho, random_state=42).str.lower())
        
        # Unifica as stopwords base com as espec√≠ficas do tema
        sw_final = STOPWORDS_PT.union(STOPWORDS_TEMA.get(cat, set()))

        # Configura√ß√£o Est√©tica Superior
        wc = WordCloud(
            width=1200, 
            height=800, 
            background_color='black', 
            stopwords=sw_final,
            min_word_length=4,  # üëà REFINAMENTO: Remove "e", "n√£o", "vc", "pq" etc.
            max_words=100, 
            collocations=False, 
            colormap='autumn' if 'Venezuela' in cat else 'cool' if 'China' in cat else 'viridis'
        ).generate(texto)
        
        filename = f"WordCloud_{cat.replace('-', '_')}_Refinada.png"
        wc.to_file(os.path.join(SAVE_PATH, filename))

    print(f"‚úÖ Sucesso! Nuvens salvas em: {SAVE_PATH}")

if __name__ == "__main__":
    gerar_nuvens_limpas()